{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch_geometric/typing.py:42: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: dlopen(/usr/local/lib/python3.11/site-packages/torch_sparse/_spmm_cpu.so, 0x0006): Symbol not found: (__ZN5torch8autograd13_wrap_outputsERKNSt3__16vectorIN2at6TensorENS1_9allocatorIS4_EEEERKNS1_13unordered_setIPN3c1010TensorImplENS1_4hashISD_EENS1_8equal_toISD_EENS5_ISD_EEEESL_NSB_8ArrayRefINSB_8optionalIS4_EEEERKNS1_10shared_ptrINS0_4NodeEEENS1_8functionIFS7_S7_S7_EEE)\n",
      "  Referenced from: '/usr/local/lib/python3.11/site-packages/torch_sparse/_spmm_cpu.so'\n",
      "  Expected in: '/usr/local/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import negative_sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyg_graph = torch.load('pyg_graph.pt')\n",
    "transform = RandomLinkSplit(is_undirected=True)\n",
    "train_data, val_data, test_data = transform(pyg_graph)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(pyg_graph.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, confusion_matrix\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embeddings and set model parameters\n",
    "embeddings = pd.read_csv('full_users_embeddings_2.csv')\n",
    "input_dim = embeddings.shape[1]\n",
    "hidden_dim = 64\n",
    "groups = pd.read_json(\"babynamesDB_groups.json\")\n",
    "groups = groups.query(\"num_users_stored > 3\")\n",
    "group_ids = groups[\"_id\"].to_list()\n",
    "num_groups = len(group_ids)\n",
    "num_layers = 2\n",
    "dropout = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb Cell 5\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=94'>95</a>\u001b[0m loss_fn \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mBCEWithLogitsLoss()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=95'>96</a>\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=97'>98</a>\u001b[0m train(model, train_data, optimizer, loss_fn)\n",
      "\u001b[1;32m/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb Cell 5\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# Negative sampling for training\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m neg_edge_index \u001b[39m=\u001b[39m negative_sampling(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m     edge_index\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39medge_index,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m     num_nodes\u001b[39m=\u001b[39mdata\u001b[39m.\u001b[39medge_index\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m     num_neg_samples\u001b[39m=\u001b[39mpyg_graph\u001b[39m.\u001b[39mnum_edges \u001b[39m-\u001b[39m data\u001b[39m.\u001b[39mnum_edges\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m     )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m link_logits \u001b[39m=\u001b[39m model(data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m link_labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([torch\u001b[39m.\u001b[39mones(pos_edge_index\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m)), \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m                          torch\u001b[39m.\u001b[39mzeros(neg_edge_index\u001b[39m.\u001b[39msize(\u001b[39m1\u001b[39m))], dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(link_logits, link_labels)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, data):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     z \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencode(data\u001b[39m.\u001b[39;49mx, data\u001b[39m.\u001b[39;49medge_index)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     link_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecode(z, data\u001b[39m.\u001b[39medge_index)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m link_logits\n",
      "\u001b[1;32m/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb Cell 5\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m x_hat \u001b[39m=\u001b[39m x\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconvs)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     x_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvs[i](x_hat, edge_index)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     x_hat \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbns[i](x_hat)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/test.ipynb#W2sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     x_hat \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(x_hat)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch_geometric/nn/conv/gcn_conv.py:211\u001b[0m, in \u001b[0;36mGCNConv.forward\u001b[0;34m(self, x, edge_index, edge_weight)\u001b[0m\n\u001b[1;32m    208\u001b[0m cache \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index\n\u001b[1;32m    209\u001b[0m \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m     edge_index, edge_weight \u001b[39m=\u001b[39m gcn_norm(  \u001b[39m# yapf: disable\u001b[39;00m\n\u001b[0;32m--> 211\u001b[0m         edge_index, edge_weight, x\u001b[39m.\u001b[39;49msize(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_dim),\n\u001b[1;32m    212\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimproved, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madd_self_loops, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflow, x\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m    213\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcached:\n\u001b[1;32m    214\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cached_edge_index \u001b[39m=\u001b[39m (edge_index, edge_weight)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'size'"
     ]
    }
   ],
   "source": [
    "# Define the GCN Model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_groups, num_layers, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        # Initialize convolutional layers\n",
    "        self.convs = torch.nn.ModuleList([GCNConv(input_dim, hidden_dim)])\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        # Output layer for group prediction\n",
    "        self.convs.append(GCNConv(hidden_dim, num_groups))  \n",
    "\n",
    "        # Initialize batch normalization layers\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    # Encoding function to generate node embeddings\n",
    "    def encode(self, x, edge_index):\n",
    "        x_hat = x\n",
    "        for i in range(len(self.convs)-1):\n",
    "            x_hat = self.convs[i](x_hat, edge_index)\n",
    "            x_hat = self.bns[i](x_hat)\n",
    "            x_hat = F.relu(x_hat)\n",
    "            x_hat = F.dropout(x_hat, self.dropout, training=self.training)\n",
    "        return self.convs[-1](x_hat, edge_index)\n",
    "\n",
    "    # Decoding function to compute edge scores\n",
    "    def decode(self, z, edge_index):\n",
    "        return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, data):\n",
    "        z = self.encode(data.x, data.edge_index)\n",
    "        link_logits = self.decode(z, data.edge_index)\n",
    "        return link_logits\n",
    "\n",
    "# Function to train the model\n",
    "def train(model, data, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Positive samples from the graph\n",
    "    pos_edge_index = data.edge_index\n",
    "\n",
    "    # Negative sampling for training\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.edge_index,\n",
    "        num_nodes=data.edge_index.shape[1],\n",
    "        num_neg_samples=pyg_graph.num_edges - data.num_edges\n",
    "        )\n",
    "\n",
    "    link_logits = model(data)\n",
    "    link_labels = torch.cat([torch.ones(pos_edge_index.size(1)), \n",
    "                             torch.zeros(neg_edge_index.size(1))], dim=0)\n",
    "    loss = loss_fn(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Function to evaluate the model\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "\n",
    "    # Positive and negative samples for testing\n",
    "    pos_edge_index = data.edge_index\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=data.edge_index,\n",
    "        num_nodes=data.edge_index.shape[1],\n",
    "        num_neg_samples=pyg_graph.num_edges - data.num_edges\n",
    "        )\n",
    "\n",
    "\n",
    "    pos_link_logits = model.encode(data.x, pos_edge_index)\n",
    "    neg_link_logits = model.encode(data.x, neg_edge_index)\n",
    "    pos_probs = torch.sigmoid(pos_link_logits).cpu().numpy()\n",
    "    neg_probs = torch.sigmoid(neg_link_logits).cpu().numpy()\n",
    "\n",
    "    # Combine positive and negative predictions\n",
    "    probs = np.concatenate([pos_probs, neg_probs])\n",
    "    labels = np.concatenate([np.ones(pos_probs.shape[0]), np.zeros(neg_probs.shape[0])])\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    auc_roc = roc_auc_score(labels, probs)\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "\n",
    "    return auc_roc, precision, recall, f1, conf_matrix\n",
    "\n",
    "# Initialize the model\n",
    "model = GCN(input_dim, hidden_dim, num_groups, num_layers, dropout)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "train(model, train_data, optimizer, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
