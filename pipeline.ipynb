{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings shape:  (369244, 768)\n",
      "sparse embeddings shape:  (370392, 1916)\n",
      "(1148, 1148)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m attrs \u001b[39m=\u001b[39m {}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m G\u001b[39m.\u001b[39mnodes():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     data \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39;49miloc[\u001b[39mint\u001b[39;49m(node)]\u001b[39m.\u001b[39;49mto_dict()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39m# if there are fields in embeddings but not in data,add and fill them with 0\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mfor\u001b[39;00m field \u001b[39min\u001b[39;00m embeddings\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mto_list():\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/series.py:1901\u001b[0m, in \u001b[0;36mSeries.to_dict\u001b[0;34m(self, into)\u001b[0m\n\u001b[1;32m   1897\u001b[0m     \u001b[39mreturn\u001b[39;00m into_c((k, maybe_box_native(v)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems())\n\u001b[1;32m   1898\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1899\u001b[0m     \u001b[39m# Not an object dtype => all types will be the same so let the default\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     \u001b[39m# indexer return native python type\u001b[39;00m\n\u001b[0;32m-> 1901\u001b[0m     \u001b[39mreturn\u001b[39;00m into_c(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import xgi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from scipy import sparse\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "groups = pd.read_json(\"babynamesDB_groups.json\")\n",
    "groups = groups.query(\"num_users_stored > 3\")\n",
    "group_ids = groups[\"_id\"].to_list()\n",
    "\n",
    "# Efficiently handle embeddings\n",
    "embeddings = pd.read_csv('full_users_embeddings_2.csv')\n",
    "# keep only the last 768 columns\n",
    "embeddings = embeddings.iloc[:, -768:]\n",
    "print('embeddings shape: ', embeddings.shape)\n",
    "# Using sparse matrix for embeddings\n",
    "sparse_embeddings = sparse.lil_matrix((len(embeddings) + len(group_ids), embeddings.shape[1] + len(group_ids)))\n",
    "print('sparse embeddings shape: ', sparse_embeddings.shape)\n",
    "# Populate the matrix\n",
    "sparse_embeddings[:len(embeddings), :embeddings.shape[1]] = embeddings.values\n",
    "\n",
    "# One-hot encoding for groups\n",
    "group_one_hot = sparse.eye(len(group_ids))\n",
    "sparse_embeddings[len(embeddings):, -len(group_ids):] = group_one_hot\n",
    "print(group_one_hot.shape)\n",
    "\n",
    "embeddings = pd.DataFrame(sparse_embeddings.todense(), columns=embeddings.columns.to_list() + group_ids)\n",
    "# set column names to range of integers\n",
    "embeddings.rename(columns={col: i for i, col in enumerate(embeddings.columns)}, inplace=True)\n",
    "G = nx.read_edgelist('graph.edgelist')\n",
    "\n",
    "attrs = {}\n",
    "for node in G.nodes():\n",
    "    data = embeddings.iloc[int(node)].to_dict()\n",
    "    # if there are fields in embeddings but not in data,add and fill them with 0\n",
    "    for field in embeddings.columns.to_list():\n",
    "        if field not in data:\n",
    "            data[field] = 0\n",
    "    attrs[int(node)] = data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.set_node_attributes(G, attrs)    \n",
    "print('node attributes set')\n",
    "pyg_graph = from_networkx(G)\n",
    "\n",
    "\n",
    "print('graph converted to pytorch geometric graph')\n",
    "\n",
    "torch.save(pyg_graph, 'pyg_graph.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pyg_graph = torch.read('pyg_graph.pt')\n",
    "data = train_test_split_edges(pyg_graph)\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_groups, num_layers, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.convs = torch.nn.ModuleList([GCNConv(input_dim, hidden_dim)])\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        self.convs.append(GCNConv(hidden_dim, num_groups))  # Output layer for group prediction\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x_hat = x\n",
    "        for i in range(len(self.convs)-1):\n",
    "            x_hat = self.convs[i](x_hat, edge_index)\n",
    "            x_hat = self.bns[i](x_hat)\n",
    "            x_hat = F.relu(x_hat)\n",
    "            x_hat = F.dropout(x_hat, self.dropout, training=self.training)\n",
    "        return self.convs[-1](x_hat, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_index):\n",
    "        return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        z = self.encode(data.x, data.train_pos_edge_index)\n",
    "        link_logits = self.decode(z, data.train_pos_edge_index)\n",
    "        return link_logits\n",
    "\n",
    "def train(model, data, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    link_logits = model(data)\n",
    "    link_labels = torch.cat([torch.ones(data.train_pos_edge_index.size(1)), \n",
    "                             torch.zeros(data.train_neg_edge_index.size(1))], dim=0)\n",
    "    loss = loss_fn(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    pos_link_logits = model.encode(data.x, data.test_pos_edge_index)\n",
    "    neg_link_logits = model.encode(data.x, data.test_neg_edge_index)\n",
    "    pos_link_probs = F.softmax(pos_link_logits, dim=1)\n",
    "    neg_link_probs = F.softmax(neg_link_logits, dim=1)\n",
    "    # Here you can calculate the evaluation metrics like AUC, Accuracy, etc.\n",
    "    return pos_link_probs, neg_link_probs\n",
    "\n",
    "input_dim = embeddings.shape[1] - 4  # Minus 4 to exclude '_id', 'one_hot', and two additional columns\n",
    "hidden_dim = 64  # Example value, you may need to tune this\n",
    "num_groups = hyperedge_list['groups'].nunique()  # Assuming this is the number of groups\n",
    "num_layers = 2   # Number of layers in GCN\n",
    "dropout = 0.5    # Dropout rate\n",
    "\n",
    "model = GCN(input_dim, hidden_dim, num_groups, num_layers, dropout)\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Learning rate may need tuning\n",
    "num_epochs = 100  # Number of epochs to train\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, data, optimizer, loss_fn)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}/{num_epochs}, Loss: {loss:.4f}')\n",
    "\n",
    "pos_link_probs, neg_link_probs = test(model, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
