{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "pyg_graph = torch.load('pyg_graph_with_features.pt')\n",
    "\n",
    "# only keep edge_intex, x, and num_nodes\n",
    "graph = Data(x=pyg_graph.x, edge_index=pyg_graph.edge_index, num_nodes=pyg_graph.num_nodes)\n",
    "\n",
    "transform = RandomLinkSplit(is_undirected=True)\n",
    "train_data, val_data, test_data = transform(graph)\n",
    "\n",
    "print('data preprocessed')\n",
    "\n",
    "# Define the GCN Model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_groups, num_layers, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        # Initialize convolutional layers\n",
    "        self.convs = torch.nn.ModuleList([GCNConv(input_dim, hidden_dim)])\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        # Output layer for group prediction\n",
    "        self.convs.append(GCNConv(hidden_dim, num_groups))  \n",
    "\n",
    "        # Initialize batch normalization layers\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    # Encoding function to generate node embeddings\n",
    "    def encode(self, x, edge_index):\n",
    "        x_hat = x\n",
    "        x_hat = self.convs[0](x_hat, edge_index)  # First convolutional layer\n",
    "        for i in range(1, len(self.convs) - 1):  # Adjusted loop\n",
    "            x_hat = self.bns[i](x_hat)  # Apply batch normalization\n",
    "            x_hat = F.relu(x_hat)  # Apply ReLU\n",
    "            # x_hat = F.dropout(x_hat, self.dropout, training=self.training)  # Apply dropout\n",
    "            x_hat = self.convs[i](x_hat, edge_index)  # Apply next convolutional layer\n",
    "        return x_hat  # Return the transformed features\n",
    "\n",
    "\n",
    "    # Decoding function to compute edge scores\n",
    "    def decode(self, z, edge_index):\n",
    "        return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.encode(x, edge_index)\n",
    "        link_logits = self.decode(z, edge_index)\n",
    "        return link_logits\n",
    "\n",
    "# Function to train the model\n",
    "def train(model, data, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pos_edge_index = data.edge_index\n",
    "    neg_edge_index = negative_sampling(edge_index=pos_edge_index, num_nodes=data.num_nodes)\n",
    "\n",
    "    # Concatenate positive and negative edges\n",
    "    total_edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=1)\n",
    "\n",
    "    link_logits = model(data.x, total_edge_index)\n",
    "    link_labels = torch.cat([torch.ones(pos_edge_index.size(1)), \n",
    "                             torch.zeros(neg_edge_index.size(1))], dim=0).to(device)\n",
    "    \n",
    "    loss = loss_fn(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Function to evaluate the model\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    pos_edge_index = data.edge_index\n",
    "    neg_edge_index = negative_sampling(edge_index=pos_edge_index, num_nodes=data.num_nodes)\n",
    "\n",
    "    total_edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=1)\n",
    "    link_logits = model(data.x, total_edge_index)\n",
    "\n",
    "    link_labels = torch.cat([torch.ones(pos_edge_index.size(1)), \n",
    "                             torch.zeros(neg_edge_index.size(1))], dim=0).to(device)\n",
    "\n",
    "    probs = torch.sigmoid(link_logits).cpu().numpy()\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    labels = link_labels.cpu().numpy()\n",
    "\n",
    "    auc_roc = roc_auc_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    \n",
    "    return auc_roc, precision, recall, f1\n",
    "\n",
    "input_dim = graph.x.shape[1]\n",
    "print(f'input_dim : {input_dim}')\n",
    "hidden_dim = 64\n",
    "\n",
    "groups = pd.read_json(\"babynamesDB_groups.json\")\n",
    "groups = groups.query(\"num_users_stored > 3\")\n",
    "group_ids = groups[\"_id\"].to_list()\n",
    "num_groups = len(group_ids)\n",
    "num_layers = 3\n",
    "dropout = 0.5\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = GCN(input_dim, hidden_dim, num_groups, num_layers, dropout)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print('GCN initialized')\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "# train on GPU if available\n",
    "\n",
    "model = model.to(device)\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "\n",
    "# Training loop with error handling and metric logging\n",
    "train_losses, val_metrics = [], []\n",
    "for epoch in range(num_epochs):\n",
    "    try:\n",
    "        train_loss = train(model, train_data, optimizer, loss_fn)\n",
    "        auc_roc, precision, recall, f1 = evaluate(model, val_data)\n",
    "        train_losses.append(train_loss)\n",
    "        val_metrics.append((auc_roc, precision, recall, f1))\n",
    "        print(f'Epoch {epoch}: Loss: {train_loss:.4f}, AUC-ROC: {auc_roc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n",
    "        scheduler.step()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception encountered: {e}\")\n",
    "        break\n",
    "\n",
    "# Writing training loss and validation metrics to a file\n",
    "with open(\"training_validation_metrics.txt\", \"w\") as file:\n",
    "    file.write(\"Epoch, Training Loss, AUC-ROC, Precision, Recall, F1-Score\\n\")\n",
    "    for i, epoch in enumerate(num_epochs):\n",
    "        train_loss = train_losses[i]\n",
    "        auc_roc, precision, recall, f1 = val_metrics[i]\n",
    "        file.write(f\"{epoch}, {train_loss:.4f}, {auc_roc:.4f}, {precision:.4f}, {recall:.4f}, {f1:.4f}\\n\")\n",
    "\n",
    "# Plot training loss and validation metrics\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "val_metrics = np.array(val_metrics)\n",
    "metrics_labels = ['AUC-ROC', 'Precision', 'Recall', 'F1-Score']\n",
    "for i, label in enumerate(metrics_labels):\n",
    "    plt.plot(epochs, val_metrics[:, i], label=label)\n",
    "plt.title('Validation Metrics Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
