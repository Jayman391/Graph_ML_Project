{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings shape:  (369244, 768)\n",
      "sparse embeddings shape:  (370392, 1916)\n",
      "(1148, 1148)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m attrs \u001b[39m=\u001b[39m {}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m G\u001b[39m.\u001b[39mnodes():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     data \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39;49miloc[\u001b[39mint\u001b[39;49m(node)]\u001b[39m.\u001b[39;49mto_dict()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39m# if there are fields in embeddings but not in data,add and fill them with 0\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mfor\u001b[39;00m field \u001b[39min\u001b[39;00m embeddings\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mto_list():\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/series.py:1901\u001b[0m, in \u001b[0;36mSeries.to_dict\u001b[0;34m(self, into)\u001b[0m\n\u001b[1;32m   1897\u001b[0m     \u001b[39mreturn\u001b[39;00m into_c((k, maybe_box_native(v)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems())\n\u001b[1;32m   1898\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1899\u001b[0m     \u001b[39m# Not an object dtype => all types will be the same so let the default\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     \u001b[39m# indexer return native python type\u001b[39;00m\n\u001b[0;32m-> 1901\u001b[0m     \u001b[39mreturn\u001b[39;00m into_c(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import xgi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "groups = pd.read_json(\"babynamesDB_groups.json\")\n",
    "groups = groups.query(\"num_users_stored > 3\")\n",
    "group_ids = groups[\"_id\"].to_list()\n",
    "\n",
    "# Efficiently handle embeddings\n",
    "embeddings = pd.read_csv('full_users_embeddings_2.csv')\n",
    "# keep only the last 768 columns\n",
    "embeddings = embeddings.iloc[:, -768:]\n",
    "print('embeddings shape: ', embeddings.shape)\n",
    "# Using sparse matrix for embeddings\n",
    "sparse_embeddings = sparse.lil_matrix((len(embeddings) + len(group_ids), embeddings.shape[1] + len(group_ids)))\n",
    "print('sparse embeddings shape: ', sparse_embeddings.shape)\n",
    "# Populate the matrix\n",
    "sparse_embeddings[:len(embeddings), :embeddings.shape[1]] = embeddings.values\n",
    "\n",
    "# One-hot encoding for groups\n",
    "group_one_hot = sparse.eye(len(group_ids))\n",
    "sparse_embeddings[len(embeddings):, -len(group_ids):] = group_one_hot\n",
    "print(group_one_hot.shape)\n",
    "\n",
    "embeddings = pd.DataFrame(sparse_embeddings.todense(), columns=embeddings.columns.to_list() + group_ids)\n",
    "# set column names to range of integers\n",
    "embeddings.rename(columns={col: i for i, col in enumerate(embeddings.columns)}, inplace=True)\n",
    "G = nx.read_edgelist('graph.edgelist')\n",
    "\n",
    "attrs = {}\n",
    "for node in G.nodes():\n",
    "    data = embeddings.iloc[int(node)].to_dict()\n",
    "    # if there are fields in embeddings but not in data,add and fill them with 0\n",
    "    for field in embeddings.columns.to_list():\n",
    "        if field not in data:\n",
    "            data[field] = 0\n",
    "    attrs[int(node)] = data \n",
    "\n",
    "nx.set_node_attributes(G, attrs)    \n",
    "print('node attributes set')\n",
    "pyg_graph = from_networkx(G)\n",
    "\n",
    "\n",
    "print('graph converted to pytorch geometric graph')\n",
    "\n",
    "torch.save(pyg_graph, 'pyg_graph.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch_geometric/typing.py:42: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: dlopen(/usr/local/lib/python3.11/site-packages/torch_sparse/_spmm_cpu.so, 0x0006): Symbol not found: (__ZN5torch8autograd13_wrap_outputsERKNSt3__16vectorIN2at6TensorENS1_9allocatorIS4_EEEERKNS1_13unordered_setIPN3c1010TensorImplENS1_4hashISD_EENS1_8equal_toISD_EENS5_ISD_EEEESL_NSB_8ArrayRefINSB_8optionalIS4_EEEERKNS1_10shared_ptrINS0_4NodeEEENS1_8functionIFS7_S7_S7_EEE)\n",
      "  Referenced from: '/usr/local/lib/python3.11/site-packages/torch_sparse/_spmm_cpu.so'\n",
      "  Expected in: '/usr/local/lib/python3.11/site-packages/torch/lib/libtorch_cpu.dylib'\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load embeddings and set model parameters\n",
    "embeddings = pd.read_csv('full_users_embeddings_2.csv')\n",
    "input_dim = embeddings.shape[1]\n",
    "hidden_dim = 64\n",
    "groups = pd.read_json(\"babynamesDB_groups.json\")\n",
    "groups = groups.query(\"num_users_stored > 3\")\n",
    "group_ids = groups[\"_id\"].to_list()\n",
    "num_groups = len(group_ids)\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# Load your graph and perform train/test split\n",
    "pyg_graph = torch.load('pyg_graph.pt')\n",
    "pyg_graph.x = torch.tensor(embeddings.values, dtype=torch.float)\n",
    "\n",
    "transform = RandomLinkSplit(is_undirected=True)\n",
    "train_data, val_data, test_data = transform(pyg_graph)\n",
    "\n",
    "# Define the GCN Model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_groups, num_layers, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        # Initialize convolutional layers\n",
    "        self.convs = torch.nn.ModuleList([GCNConv(input_dim, hidden_dim)])\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        # Output layer for group prediction\n",
    "        self.convs.append(GCNConv(hidden_dim, num_groups))  \n",
    "\n",
    "        # Initialize batch normalization layers\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    # Encoding function to generate node embeddings\n",
    "    def encode(self, x, edge_index):\n",
    "        x_hat = x\n",
    "        for i in range(len(self.convs)-1):\n",
    "            x_hat = self.convs[i](x_hat, edge_index)\n",
    "            x_hat = self.bns[i](x_hat)\n",
    "            x_hat = F.relu(x_hat)\n",
    "            x_hat = F.dropout(x_hat, self.dropout, training=self.training)\n",
    "        return self.convs[-1](x_hat, edge_index)\n",
    "\n",
    "    # Decoding function to compute edge scores\n",
    "    def decode(self, z, edge_index):\n",
    "        return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, data):\n",
    "        z = self.encode(data.x, data.edge_index)\n",
    "        link_logits = self.decode(z, data.edge_index)\n",
    "        return link_logits\n",
    "\n",
    "# Function to train the model\n",
    "def train(model, data, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Positive samples from the graph\n",
    "    pos_edge_index = train_data.edge_index\n",
    "\n",
    "    # Negative sampling for training\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=train_data.edge_index,\n",
    "        num_nodes=train_data.edge_index.shape[1],\n",
    "        num_neg_samples=np.abs(pyg_graph.num_edges - train_data.num_edges)\n",
    "        )\n",
    "\n",
    "    link_logits = model(data)\n",
    "    link_labels = torch.cat([torch.ones(pos_edge_index.size(1)), \n",
    "                             torch.zeros(neg_edge_index.size(1))], dim=0)\n",
    "    loss = loss_fn(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Function to evaluate the model\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "\n",
    "    # Positive and negative samples for testing\n",
    "    pos_edge_index = test_data.edge_index\n",
    "    neg_edge_index = negative_sampling(\n",
    "        edge_index=test_data.edge_index,\n",
    "        num_nodes=test_data.edge_index.shape[1],\n",
    "        num_neg_samples=np.abs(pyg_graph.num_edges - test_data.num_edges)\n",
    "        )\n",
    "\n",
    "\n",
    "    pos_link_logits = model.encode(data.x, pos_edge_index)\n",
    "    neg_link_logits = model.encode(data.x, neg_edge_index)\n",
    "    pos_probs = torch.sigmoid(pos_link_logits).cpu().numpy()\n",
    "    neg_probs = torch.sigmoid(neg_link_logits).cpu().numpy()\n",
    "\n",
    "    # Combine positive and negative predictions\n",
    "    probs = np.concatenate([pos_probs, neg_probs])\n",
    "    labels = np.concatenate([np.ones(pos_probs.shape[0]), np.zeros(neg_probs.shape[0])])\n",
    "    \n",
    "    # Calculate evaluation metrics\n",
    "    auc_roc = roc_auc_score(labels, probs)\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    conf_matrix = confusion_matrix(labels, preds)\n",
    "\n",
    "    return auc_roc, precision, recall, f1, conf_matrix\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = GCN(input_dim, hidden_dim, num_groups, num_layers, dropout)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, train_data, optimizer, loss_fn)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}/{num_epochs}, Loss: {loss:.4f}')\n",
    "\n",
    "# Evaluate the model \n",
    "auc_roc, precision, recall, f1, conf_matrix = evaluate(model, test_data)\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
