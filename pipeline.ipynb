{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings shape:  (369244, 768)\n",
      "sparse embeddings shape:  (370392, 1916)\n",
      "(1148, 1148)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m attrs \u001b[39m=\u001b[39m {}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m G\u001b[39m.\u001b[39mnodes():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     data \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39;49miloc[\u001b[39mint\u001b[39;49m(node)]\u001b[39m.\u001b[39;49mto_dict()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39m# if there are fields in embeddings but not in data,add and fill them with 0\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mfor\u001b[39;00m field \u001b[39min\u001b[39;00m embeddings\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mto_list():\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/series.py:1901\u001b[0m, in \u001b[0;36mSeries.to_dict\u001b[0;34m(self, into)\u001b[0m\n\u001b[1;32m   1897\u001b[0m     \u001b[39mreturn\u001b[39;00m into_c((k, maybe_box_native(v)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems())\n\u001b[1;32m   1898\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1899\u001b[0m     \u001b[39m# Not an object dtype => all types will be the same so let the default\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     \u001b[39m# indexer return native python type\u001b[39;00m\n\u001b[0;32m-> 1901\u001b[0m     \u001b[39mreturn\u001b[39;00m into_c(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import xgi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "groups = pd.read_json(\"babynamesDB_groups.json\")\n",
    "groups = groups.query(\"num_users_stored > 3\")\n",
    "group_ids = groups[\"_id\"].to_list()\n",
    "\n",
    "# Efficiently handle embeddings\n",
    "embeddings = pd.read_csv('full_users_embeddings_2.csv')\n",
    "# keep only the last 768 columns\n",
    "embeddings = embeddings.iloc[:, -768:]\n",
    "print('embeddings shape: ', embeddings.shape)\n",
    "# Using sparse matrix for embeddings\n",
    "sparse_embeddings = sparse.lil_matrix((len(embeddings) + len(group_ids), embeddings.shape[1] + len(group_ids)))\n",
    "print('sparse embeddings shape: ', sparse_embeddings.shape)\n",
    "# Populate the matrix\n",
    "sparse_embeddings[:len(embeddings), :embeddings.shape[1]] = embeddings.values\n",
    "\n",
    "# One-hot encoding for groups\n",
    "group_one_hot = sparse.eye(len(group_ids))\n",
    "sparse_embeddings[len(embeddings):, -len(group_ids):] = group_one_hot\n",
    "print(group_one_hot.shape)\n",
    "\n",
    "embeddings = pd.DataFrame(sparse_embeddings.todense(), columns=embeddings.columns.to_list() + group_ids)\n",
    "# set column names to range of integers\n",
    "embeddings.rename(columns={col: i for i, col in enumerate(embeddings.columns)}, inplace=True)\n",
    "G = nx.read_edgelist('graph.edgelist')\n",
    "\n",
    "attrs = {}\n",
    "for node in G.nodes():\n",
    "    data = embeddings.iloc[int(node)].to_dict()\n",
    "    # if there are fields in embeddings but not in data,add and fill them with 0\n",
    "    for field in embeddings.columns.to_list():\n",
    "        if field not in data:\n",
    "            data[field] = 0\n",
    "    attrs[int(node)] = data \n",
    "\n",
    "nx.set_node_attributes(G, attrs)    \n",
    "print('node attributes set')\n",
    "pyg_graph = from_networkx(G)\n",
    "\n",
    "\n",
    "print('graph converted to pytorch geometric graph')\n",
    "\n",
    "torch.save(pyg_graph, 'pyg_graph.pt')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "pyg_graph = torch.load('pyg_graph_with_features.pt')\n",
    "\n",
    "# only keep edge_intex, x, and num_nodes\n",
    "graph = Data(x=pyg_graph.x, edge_index=pyg_graph.edge_index, num_nodes=pyg_graph.num_nodes)\n",
    "\n",
    "transform = RandomLinkSplit(is_undirected=True)\n",
    "train_data, val_data, test_data = transform(graph)\n",
    "\n",
    "print('data preprocessed')\n",
    "\n",
    "# Define the GCN Model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_groups, num_layers, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        # Initialize convolutional layers\n",
    "        self.convs = torch.nn.ModuleList([GCNConv(input_dim, hidden_dim)])\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        # Output layer for group prediction\n",
    "        self.convs.append(GCNConv(hidden_dim, num_groups))  \n",
    "\n",
    "        # Initialize batch normalization layers\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    # Encoding function to generate node embeddings\n",
    "    def encode(self, x, edge_index):\n",
    "        x_hat = x\n",
    "        x_hat = self.convs[0](x_hat, edge_index)  # First convolutional layer\n",
    "        for i in range(1, len(self.convs) - 1):  # Adjusted loop\n",
    "            x_hat = self.bns[i](x_hat)  # Apply batch normalization\n",
    "            x_hat = F.relu(x_hat)  # Apply ReLU\n",
    "            x_hat = F.dropout(x_hat, self.dropout, training=self.training)  # Apply dropout\n",
    "            x_hat = self.convs[i](x_hat, edge_index)  # Apply next convolutional layer\n",
    "        return x_hat  # Return the transformed features\n",
    "\n",
    "\n",
    "    # Decoding function to compute edge scores\n",
    "    def decode(self, z, edge_index):\n",
    "        return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, x, edge_index):\n",
    "        z = self.encode(x, edge_index)\n",
    "        link_logits = self.decode(z, edge_index)\n",
    "        return link_logits\n",
    "\n",
    "# Function to train the model\n",
    "def train(model, data, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    pos_edge_index = data.edge_index\n",
    "    neg_edge_index = negative_sampling(edge_index=pos_edge_index, num_nodes=data.num_nodes)\n",
    "\n",
    "    # Concatenate positive and negative edges\n",
    "    total_edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=1)\n",
    "\n",
    "    link_logits = model(data.x, total_edge_index)\n",
    "    link_labels = torch.cat([torch.ones(pos_edge_index.size(1)), \n",
    "                             torch.zeros(neg_edge_index.size(1))], dim=0).to(device)\n",
    "    \n",
    "    loss = loss_fn(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "# Function to evaluate the model\n",
    "@torch.no_grad()\n",
    "def evaluate(model, data):\n",
    "    model.eval()\n",
    "    pos_edge_index = data.edge_index\n",
    "    neg_edge_index = negative_sampling(edge_index=pos_edge_index, num_nodes=data.num_nodes)\n",
    "\n",
    "    total_edge_index = torch.cat([pos_edge_index, neg_edge_index], dim=1)\n",
    "    link_logits = model(data.x, total_edge_index)\n",
    "\n",
    "    link_labels = torch.cat([torch.ones(pos_edge_index.size(1)), \n",
    "                             torch.zeros(neg_edge_index.size(1))], dim=0).to(device)\n",
    "\n",
    "    probs = torch.sigmoid(link_logits).cpu().numpy()\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    labels = link_labels.cpu().numpy()\n",
    "\n",
    "    auc_roc = roc_auc_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    \n",
    "    return auc_roc, precision, recall, f1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "input_dim = graph.x.shape[1]\n",
    "print(f'input_dim : {input_dim}')\n",
    "hidden_dim = 64\n",
    "\n",
    "groups = pd.read_json(\"babynamesDB_groups.json\")\n",
    "groups = groups.query(\"num_users_stored > 3\")\n",
    "group_ids = groups[\"_id\"].to_list()\n",
    "num_groups = len(group_ids)\n",
    "num_layers = 3\n",
    "dropout = 0.5\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = GCN(input_dim, hidden_dim, num_groups, num_layers, dropout)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "print('GCN initialized')\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# train on GPU if available\n",
    "\n",
    "model = model.to(device)\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "\n",
    "# Training loop with error handling and metric logging\n",
    "train_losses, val_metrics = [], []\n",
    "for epoch in range(num_epochs):\n",
    "    try:\n",
    "        train_loss = train(model, train_data, optimizer, loss_fn)\n",
    "        auc_roc, precision, recall, f1 = evaluate(model, val_data)\n",
    "        train_losses.append(train_loss)\n",
    "        val_metrics.append((auc_roc, precision, recall, f1))\n",
    "        print(f'Epoch {epoch}: Loss: {train_loss:.4f}, AUC-ROC: {auc_roc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}')\n",
    "        scheduler.step()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Exception encountered: {e}\")\n",
    "        break\n",
    "\n",
    "# Writing training loss and validation metrics to a file\n",
    "with open(\"training_validation_metrics.txt\", \"w\") as file:\n",
    "    file.write(\"Epoch, Training Loss, AUC-ROC, Precision, Recall, F1-Score\\n\")\n",
    "    for i, epoch in enumerate(num_epochs):\n",
    "        train_loss = train_losses[i]\n",
    "        auc_roc, precision, recall, f1 = val_metrics[i]\n",
    "        file.write(f\"{epoch}, {train_loss:.4f}, {auc_roc:.4f}, {precision:.4f}, {recall:.4f}, {f1:.4f}\\n\")\n",
    "\n",
    "# Plot training loss and validation metrics\n",
    "epochs = range(1, len(train_losses) + 1)\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "val_metrics = np.array(val_metrics)\n",
    "metrics_labels = ['AUC-ROC', 'Precision', 'Recall', 'F1-Score']\n",
    "for i, label in enumerate(metrics_labels):\n",
    "    plt.plot(epochs, val_metrics[:, i], label=label)\n",
    "plt.title('Validation Metrics Over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Metric')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
