{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings shape:  (369244, 768)\n",
      "sparse embeddings shape:  (370392, 1916)\n",
      "(1148, 1148)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb Cell 1\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m attrs \u001b[39m=\u001b[39m {}\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m \u001b[39mfor\u001b[39;00m node \u001b[39min\u001b[39;00m G\u001b[39m.\u001b[39mnodes():\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m     data \u001b[39m=\u001b[39m embeddings\u001b[39m.\u001b[39;49miloc[\u001b[39mint\u001b[39;49m(node)]\u001b[39m.\u001b[39;49mto_dict()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m     \u001b[39m# if there are fields in embeddings but not in data,add and fill them with 0\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/user/Desktop/babycenter_graph_ml_work/pipeline.ipynb#W0sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     \u001b[39mfor\u001b[39;00m field \u001b[39min\u001b[39;00m embeddings\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mto_list():\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/pandas/core/series.py:1901\u001b[0m, in \u001b[0;36mSeries.to_dict\u001b[0;34m(self, into)\u001b[0m\n\u001b[1;32m   1897\u001b[0m     \u001b[39mreturn\u001b[39;00m into_c((k, maybe_box_native(v)) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems())\n\u001b[1;32m   1898\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1899\u001b[0m     \u001b[39m# Not an object dtype => all types will be the same so let the default\u001b[39;00m\n\u001b[1;32m   1900\u001b[0m     \u001b[39m# indexer return native python type\u001b[39;00m\n\u001b[0;32m-> 1901\u001b[0m     \u001b[39mreturn\u001b[39;00m into_c(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import xgi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import sparse\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "groups = pd.read_json(\"babynamesDB_groups.json\")\n",
    "groups = groups.query(\"num_users_stored > 3\")\n",
    "group_ids = groups[\"_id\"].to_list()\n",
    "\n",
    "# Efficiently handle embeddings\n",
    "embeddings = pd.read_csv('full_users_embeddings_2.csv')\n",
    "# keep only the last 768 columns\n",
    "embeddings = embeddings.iloc[:, -768:]\n",
    "print('embeddings shape: ', embeddings.shape)\n",
    "# Using sparse matrix for embeddings\n",
    "sparse_embeddings = sparse.lil_matrix((len(embeddings) + len(group_ids), embeddings.shape[1] + len(group_ids)))\n",
    "print('sparse embeddings shape: ', sparse_embeddings.shape)\n",
    "# Populate the matrix\n",
    "sparse_embeddings[:len(embeddings), :embeddings.shape[1]] = embeddings.values\n",
    "\n",
    "# One-hot encoding for groups\n",
    "group_one_hot = sparse.eye(len(group_ids))\n",
    "sparse_embeddings[len(embeddings):, -len(group_ids):] = group_one_hot\n",
    "print(group_one_hot.shape)\n",
    "\n",
    "embeddings = pd.DataFrame(sparse_embeddings.todense(), columns=embeddings.columns.to_list() + group_ids)\n",
    "# set column names to range of integers\n",
    "embeddings.rename(columns={col: i for i, col in enumerate(embeddings.columns)}, inplace=True)\n",
    "G = nx.read_edgelist('graph.edgelist')\n",
    "\n",
    "attrs = {}\n",
    "for node in G.nodes():\n",
    "    data = embeddings.iloc[int(node)].to_dict()\n",
    "    # if there are fields in embeddings but not in data,add and fill them with 0\n",
    "    for field in embeddings.columns.to_list():\n",
    "        if field not in data:\n",
    "            data[field] = 0\n",
    "    attrs[int(node)] = data \n",
    "\n",
    "nx.set_node_attributes(G, attrs)    \n",
    "print('node attributes set')\n",
    "pyg_graph = from_networkx(G)\n",
    "\n",
    "\n",
    "print('graph converted to pytorch geometric graph')\n",
    "\n",
    "torch.save(pyg_graph, 'pyg_graph.pt')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "from torch_geometric.utils import negative_sampling\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_fscore_support, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# use cuda\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "# Load embeddings and set model parameters\n",
    "embeddings = pd.read_csv('data/full_users_embeddings_2.csv')\n",
    "input_dim = embeddings.shape[1]\n",
    "hidden_dim = 64\n",
    "groups = pd.read_json(\"babynamesDB_groups.json\")\n",
    "groups = groups.query(\"num_users_stored > 3\")\n",
    "group_ids = groups[\"_id\"].to_list()\n",
    "num_groups = len(group_ids)\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "# Load your graph and perform train/test split\n",
    "pyg_graph = torch.load('pyg_graph.pt')\n",
    "pyg_graph.x = torch.tensor(embeddings.values, dtype=torch.float)\n",
    "\n",
    "transform = RandomLinkSplit(is_undirected=True)\n",
    "train_data, val_data, test_data = transform(pyg_graph)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=False)\n",
    "\n",
    "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "\n",
    "# Define the GCN Model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_groups, num_layers, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        # Initialize convolutional layers\n",
    "        self.convs = torch.nn.ModuleList([GCNConv(input_dim, hidden_dim)])\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        # Output layer for group prediction\n",
    "        self.convs.append(GCNConv(hidden_dim, num_groups))  \n",
    "\n",
    "        # Initialize batch normalization layers\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    # Encoding function to generate node embeddings\n",
    "    def encode(self, x, edge_index):\n",
    "        x_hat = x\n",
    "        for i in range(len(self.convs)-1):\n",
    "            x_hat = self.convs[i](x_hat, edge_index)\n",
    "            x_hat = self.bns[i](x_hat)\n",
    "            x_hat = F.relu(x_hat)\n",
    "            x_hat = F.dropout(x_hat, self.dropout, training=self.training)\n",
    "        return self.convs[-1](x_hat, edge_index)\n",
    "\n",
    "    # Decoding function to compute edge scores\n",
    "    def decode(self, z, edge_index):\n",
    "        return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, data):\n",
    "        z = self.encode(data.x, data.edge_index)\n",
    "        link_logits = self.decode(z, data.edge_index)\n",
    "        return link_logits\n",
    "\n",
    "# Function to train the model\n",
    "def train(model, loader, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Positive and Negative sampling for each batch\n",
    "        pos_edge_index = batch.edge_index\n",
    "        neg_edge_index = negative_sampling(edge_index=pos_edge_index, num_nodes=batch.x.size(0))\n",
    "\n",
    "        # Forward pass\n",
    "        link_logits = model(batch)\n",
    "        link_labels = torch.cat([torch.ones(pos_edge_index.size(1)), \n",
    "                                 torch.zeros(neg_edge_index.size(1))], dim=0).to(device)\n",
    "\n",
    "        # Loss calculation\n",
    "        loss = loss_fn(link_logits, link_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Function to evaluate the model\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    total_auc_roc, total_precision, total_recall, total_f1 = 0, 0, 0, 0\n",
    "    for batch in loader:\n",
    "        # Positive and Negative sampling for each batch\n",
    "        pos_edge_index = batch.edge_index\n",
    "        neg_edge_index = negative_sampling(edge_index=pos_edge_index, num_nodes=batch.x.size(0))\n",
    "\n",
    "        pos_link_logits = model.encode(batch.x, pos_edge_index)\n",
    "        neg_link_logits = model.encode(batch.x, neg_edge_index)\n",
    "        pos_probs = torch.sigmoid(pos_link_logits).numpy()\n",
    "        neg_probs = torch.sigmoid(neg_link_logits).numpy()\n",
    "\n",
    "        probs = np.concatenate([pos_probs, neg_probs])\n",
    "        labels = np.concatenate([np.ones(pos_probs.shape[0]), np.zeros(neg_probs.shape[0])])\n",
    "\n",
    "        auc_roc = roc_auc_score(labels, probs)\n",
    "        preds = (probs > 0.5).astype(int)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "        \n",
    "        total_auc_roc += auc_roc\n",
    "        total_precision += precision\n",
    "        total_recall += recall\n",
    "        total_f1 += f1\n",
    "    \n",
    "    num_batches = len(loader)\n",
    "    return total_auc_roc / num_batches, total_precision / num_batches, total_recall / num_batches, total_f1 / num_batches\n",
    "\n",
    "\n",
    "\n",
    "# Initialize the model\n",
    "model = GCN(input_dim, hidden_dim, num_groups, num_layers, dropout)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "# train on GPU if available\n",
    "\n",
    "model = model.to(device)\n",
    "loss_fn = loss_fn.to(device)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, train_loader, optimizer, loss_fn)\n",
    "    print(f'Epoch {epoch}: Loss: {loss:.4f}')\n",
    "\n",
    "# Evaluate the model\n",
    "auc_roc, precision, recall, f1 = evaluate(model, test_loader)\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "# Writing metrics to a file\n",
    "with open(\"evaluation_metrics.txt\", \"w\") as file:\n",
    "    file.write(f\"AUC-ROC: {auc_roc:.4f}\\n\")\n",
    "    file.write(f\"Precision: {precision:.4f}\\n\")\n",
    "    file.write(f\"Recall: {recall:.4f}\\n\")\n",
    "    file.write(f\"F1-Score: {f1:.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
