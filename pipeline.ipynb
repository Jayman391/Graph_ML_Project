{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperedge list shape:  (568386, 2)\n",
      "Hypergraph loaded\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import xgi\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils.convert import from_networkx\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.utils import train_test_split_edges\n",
    "from scipy import sparse\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "groups = pd.read_json(\"babynamesDB_groups.json\")\n",
    "groups = groups.query(\"num_users_stored > 3\")\n",
    "group_ids = groups[\"_id\"].to_list()\n",
    "\n",
    "# Efficiently handle embeddings\n",
    "embeddings = pd.read_csv('data/full_users_embeddings_2.csv')\n",
    "# keep only the last 768 columns\n",
    "embeddings = embeddings.iloc[:, -768:]\n",
    "print('embeddings shape: ', embeddings.shape)\n",
    "# Using sparse matrix for embeddings\n",
    "sparse_embeddings = sparse.lil_matrix((len(embeddings) + len(group_ids), embeddings.shape[1] + len(group_ids)))\n",
    "print('sparse embeddings shape: ', sparse_embeddings.shape)\n",
    "# Populate the matrix\n",
    "sparse_embeddings[:len(embeddings), :embeddings.shape[1]] = embeddings.values\n",
    "\n",
    "# One-hot encoding for groups\n",
    "group_one_hot = sparse.eye(len(group_ids))\n",
    "sparse_embeddings[len(embeddings):, -len(group_ids):] = group_one_hot\n",
    "print(group_one_hot.shape)\n",
    "\n",
    "embeddings = pd.DataFrame(sparse_embeddings.todense(), columns=embeddings.columns.to_list() + group_ids)\n",
    "# set column names to range of integers\n",
    "embeddings.rename(columns={col: i for i, col in enumerate(embeddings.columns)}, inplace=True)\n",
    "G = nx.read_edgelist('graph.edgelist')\n",
    "\n",
    "attrs = {}\n",
    "for node in G.nodes():\n",
    "    data = embeddings.iloc[int(node)].to_dict()\n",
    "    # if there are fields in embeddings but not in data,add and fill them with 0\n",
    "    for field in embeddings.columns.to_list():\n",
    "        if field not in data:\n",
    "            data[field] = 0\n",
    "    attrs[int(node)] = data \n",
    "\n",
    "nx.set_node_attributes(G, attrs)    \n",
    "print('node attributes set')\n",
    "\n",
    "pyg_graph = from_networkx(G)\n",
    "\n",
    "\n",
    "print('graph converted to pytorch geometric graph')\n",
    "\n",
    "torch.save(pyg_graph, 'pyg_graph.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyg_graph = torch.read('pyg_graph.pt')\n",
    "data = train_test_split_edges(pyg_graph)\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_groups, num_layers, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        \n",
    "        # Convolutional layers\n",
    "        self.convs = torch.nn.ModuleList([GCNConv(input_dim, hidden_dim)])\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(GCNConv(hidden_dim, hidden_dim))\n",
    "        self.convs.append(GCNConv(hidden_dim, num_groups))  # Output layer for group prediction\n",
    "\n",
    "        # Batch normalization layers\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.bns.append(torch.nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def encode(self, x, edge_index):\n",
    "        x_hat = x\n",
    "        for i in range(len(self.convs)-1):\n",
    "            x_hat = self.convs[i](x_hat, edge_index)\n",
    "            x_hat = self.bns[i](x_hat)\n",
    "            x_hat = F.relu(x_hat)\n",
    "            x_hat = F.dropout(x_hat, self.dropout, training=self.training)\n",
    "        return self.convs[-1](x_hat, edge_index)\n",
    "\n",
    "    def decode(self, z, edge_index):\n",
    "        return (z[edge_index[0]] * z[edge_index[1]]).sum(dim=-1)\n",
    "\n",
    "    def forward(self, data):\n",
    "        z = self.encode(data.x, data.train_pos_edge_index)\n",
    "        link_logits = self.decode(z, data.train_pos_edge_index)\n",
    "        return link_logits\n",
    "\n",
    "def train(model, data, optimizer, loss_fn):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    link_logits = model(data)\n",
    "    link_labels = torch.cat([torch.ones(data.train_pos_edge_index.size(1)), \n",
    "                             torch.zeros(data.train_neg_edge_index.size(1))], dim=0)\n",
    "    loss = loss_fn(link_logits, link_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, data):\n",
    "    model.eval()\n",
    "    pos_link_logits = model.encode(data.x, data.test_pos_edge_index)\n",
    "    neg_link_logits = model.encode(data.x, data.test_neg_edge_index)\n",
    "    pos_link_probs = F.softmax(pos_link_logits, dim=1)\n",
    "    neg_link_probs = F.softmax(neg_link_logits, dim=1)\n",
    "    # Here you can calculate the evaluation metrics like AUC, Accuracy, etc.\n",
    "    return pos_link_probs, neg_link_probs\n",
    "\n",
    "input_dim = embeddings.shape[1] - 4  # Minus 4 to exclude '_id', 'one_hot', and two additional columns\n",
    "hidden_dim = 64  # Example value, you may need to tune this\n",
    "num_groups = hyperedge_list['groups'].nunique()  # Assuming this is the number of groups\n",
    "num_layers = 2   # Number of layers in GCN\n",
    "dropout = 0.5    # Dropout rate\n",
    "\n",
    "model = GCN(input_dim, hidden_dim, num_groups, num_layers, dropout)\n",
    "\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)  # Learning rate may need tuning\n",
    "num_epochs = 100  # Number of epochs to train\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    loss = train(model, data, optimizer, loss_fn)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}/{num_epochs}, Loss: {loss:.4f}')\n",
    "\n",
    "pos_link_probs, neg_link_probs = test(model, data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
